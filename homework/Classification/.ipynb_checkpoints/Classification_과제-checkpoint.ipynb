{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9ff826",
   "metadata": {},
   "source": [
    "# 분류 과제\n",
    "\n",
    "- **titanic 데이터셋에서 '탑승한 항구(Embarked)'를 예측하는 다중분류 모델을 만들어봅니다.**\n",
    "- Embarked : Southampton(1), Cherbourg(2), Queenstown(3) \n",
    "\n",
    "- 로지스틱 회귀\n",
    "- 의사결정나무\n",
    "- 서포트벡터머신\n",
    "- kNN\n",
    "- 앙상블\n",
    "- 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8974b",
   "metadata": {},
   "source": [
    "---\n",
    "## *과제1*\n",
    "***당신은 이메일 스팸 필터를 만들고 테스트 중입니다. 필터는 100개의 스팸 이메일 중 72개를 스팸으로 식별했고, 실제 스팸이 아닌 이메일 200개 중 18개를 스팸으로 잘못 식별했습니다. 또한 필터는 스팸 이메일 28개를 놓쳤습니다. 이 때, 스팸 필터의 precision, recall, F-1 score를 계산하세요. 그리고 이 문제에서 precision과 recall 중 더 중요하다고 생각하는 지표와 그 이유를 간략히 서술하세요. (지표는 반올림하여 소수점 둘째 자리까지 표기하세요. 여기서 TP는 스팸 이메일을 양성으로 판단한 건을 의미합니다)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62752bbb",
   "metadata": {},
   "source": [
    "***답*** : \n",
    "\n",
    "---\n",
    "\n",
    "TP = 72 (스팸으로 예측했는데 맞음)\n",
    "FP = 18 (스팸으로 예측했는데 틀림)\n",
    "TN = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19b92a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 설치\n",
    "%pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a0817",
   "metadata": {},
   "source": [
    "---\n",
    "## 데이터 불러오기 ~ 피쳐엔지니어링 (실습과 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91f4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/titanic.csv\")\n",
    "\n",
    "df['Initial'] = 0 # initial 컬럼을 만들고 일시적으로 값을 0으로 초기화\n",
    "for index, row in df.iterrows():\n",
    "    initial_search = row['Name'].split(',')[1].split('.')[0].strip() # Name 컬럼에서 .(dot)을 기준으로 알파벳 문자열 추출\n",
    "    df.at[index, 'Initial'] = initial_search\n",
    "\n",
    "# 유추 가능한 값들로 대체하고, 흔하지 않은 Initial들은 Other로 대체하겠습니다.\n",
    "df['Initial'].replace([\n",
    "    'Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col',\n",
    "    'Rev', 'Capt', 'Sir', 'Don','the Countess' \n",
    "], [\n",
    "    'Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other',\n",
    "    'Other', 'Mr', 'Mr', 'Mr', 'Other'\n",
    "],\n",
    "    inplace=True)\n",
    "\n",
    "# 결측값을 Initial별 평균값으로 대체\n",
    "df.loc[(df.Age.isnull()) & (df.Initial == 'Mr'), 'Age'] = 33\n",
    "df.loc[(df.Age.isnull()) & (df.Initial == 'Mrs'), 'Age'] = 36\n",
    "df.loc[(df.Age.isnull()) & (df.Initial == 'Master'), 'Age'] = 5\n",
    "df.loc[(df.Age.isnull()) & (df.Initial == 'Miss'), 'Age'] = 22\n",
    "df.loc[(df.Age.isnull()) & (df.Initial == 'Other'), 'Age'] = 46\n",
    "\n",
    "df.dropna(subset=['Embarked'], inplace=True)\n",
    "df.drop(['Cabin','Name','PassengerId','Ticket'], axis=1, inplace=True)\n",
    "\n",
    "df['Relatives'] = df[\"SibSp\"] + df[\"Parch\"]\n",
    "\n",
    "# Sex 열 인코딩\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "# Age 열을 10년 단위로 나누어 인코딩\n",
    "df['Age'] = (df['Age'] // 10).astype(int)\n",
    "# Fare 열을 9분위로 구간화하고 인코딩\n",
    "df['Fare'] = pd.qcut(df['Fare'], q=9, labels=range(9))\n",
    "# Embarked 열 인코딩\n",
    "df['Embarked'] = df['Embarked'].map({'S': 1, 'C': 2, 'Q': 3})\n",
    "# Initial 열 인코딩\n",
    "initial_mapping = {'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Other':4}\n",
    "df['Initial'] = df['Initial'].map(initial_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f761886",
   "metadata": {},
   "source": [
    "---\n",
    "## 로지스틱 회귀 : 단일모델 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d993b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a938219d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱회귀 모델의 정확도: 0.73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      1.00      0.84       130\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.73       178\n",
      "   macro avg       0.24      0.33      0.28       178\n",
      "weighted avg       0.53      0.73      0.62       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 데이터셋을 특성(features)과 타겟 변수(target)로 분리\n",
    "X = df.drop('Embarked', axis=1)  \n",
    "y = df['Embarked']\n",
    "\n",
    "# 데이터를 훈련 세트와 테스트 세트로 나누기 (여기서는 테스트 세트가 전체의 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)       \n",
    "\n",
    "# 로지스틱회귀 모델 객체 생성\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# 훈련 데이터로 모델 학습\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "# 테스트 세트의 예측 결과 생성\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# 정확도(accuracy) 계산과 분류 보고서 출력\n",
    "accuracy = accuracy_score(y_test, y_pred)  # 정확도 계산\n",
    "report = classification_report(y_test, y_pred)  # 분류 보고서 생성\n",
    "\n",
    "# 정확도와 분류 보고서 출력\n",
    "print(f'로지스틱회귀 모델의 정확도: {accuracy:.2f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010eb258",
   "metadata": {},
   "source": [
    "---\n",
    "## *과제2 TF*\n",
    "***1. 위 코드에서 Scaler는 독립변수(feature)와 종속변수(target)에 적용되어 이들이 동일한 스케일을 갖게 한다.***\n",
    "\n",
    "***2. 위 코드에서 Scaler는 독립변수(feature)의 test데이터를 변환하는 시점에 스케일링 매개변수(최소값, 최대값)를 학습한다.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171f95f",
   "metadata": {},
   "source": [
    "***답*** : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa787a38",
   "metadata": {},
   "source": [
    "---\n",
    "## 의사결정트리 : 단일모델 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265f826f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 모델의 정확도: 0.702247191011236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.88      0.82       130\n",
      "           2       0.38      0.22      0.28        36\n",
      "           3       0.38      0.25      0.30        12\n",
      "\n",
      "    accuracy                           0.70       178\n",
      "   macro avg       0.51      0.45      0.47       178\n",
      "weighted avg       0.66      0.70      0.67       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree 모델 생성 및 학습\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Decision Tree 예측 결과 생성\n",
    "tree_pred = tree_model.predict(X_test_scaled)\n",
    "\n",
    "# Decision Tree 정확도 및 분류 보고서\n",
    "tree_accuracy = accuracy_score(y_test, tree_pred)\n",
    "tree_report = classification_report(y_test, tree_pred)\n",
    "\n",
    "print(\"Decision Tree 모델의 정확도:\", tree_accuracy)\n",
    "print(tree_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f46153",
   "metadata": {},
   "source": [
    "---\n",
    "## 서포트벡터머신 : 단일모델 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe72ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 모델의 정확도: 0.7415730337078652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.99      0.85       130\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.75      0.25      0.38        12\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.50      0.41      0.41       178\n",
      "weighted avg       0.59      0.74      0.65       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM 모델 생성 및 학습\n",
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# SVM 예측 결과 생성\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# SVM 정확도 및 분류 보고서\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_report = classification_report(y_test, svm_pred)\n",
    "\n",
    "print(\"SVM 모델의 정확도:\", svm_accuracy)\n",
    "print(svm_report)\n",
    "\n",
    "## kNN(k-Nearest Neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458949e0",
   "metadata": {},
   "source": [
    "---\n",
    "## kNN : 단일모델 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f8167c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN 모델의 정확도 :  0.7078651685393258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.88      0.82       130\n",
      "           2       0.38      0.25      0.30        36\n",
      "           3       0.50      0.25      0.33        12\n",
      "\n",
      "    accuracy                           0.71       178\n",
      "   macro avg       0.55      0.46      0.48       178\n",
      "weighted avg       0.67      0.71      0.68       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# kNN 모델 생성\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# 모델 학습\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 예측 수행\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# 정확도와 분류 보고서 계산\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_report = classification_report(y_test, knn_pred)\n",
    "\n",
    "print(\"kNN 모델의 정확도 : \", knn_accuracy)\n",
    "print(knn_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e1c3c",
   "metadata": {},
   "source": [
    "---\n",
    "## *과제3*\n",
    "***위 다중분류의 지표들을 보고, '1번 항구라고 예측한 모든 건에 대하여 실제 1번 항구를 예측한 건의 비율'은 어느 지표와 관련이 있는지 서술하시오.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f93f6",
   "metadata": {},
   "source": [
    "***답*** : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e9b04",
   "metadata": {},
   "source": [
    "---\n",
    "## 보팅(Voting) : 앙상블 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e603475a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하드보팅의 정확도 :  0.7415730337078652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.99      0.85       130\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.75      0.25      0.38        12\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.50      0.41      0.41       178\n",
      "weighted avg       0.59      0.74      0.65       178\n",
      "\n",
      "소프트보팅의 정확도 :  0.7134831460674157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.92      0.83       130\n",
      "           2       0.33      0.11      0.17        36\n",
      "           3       0.50      0.25      0.33        12\n",
      "\n",
      "    accuracy                           0.71       178\n",
      "   macro avg       0.53      0.43      0.44       178\n",
      "weighted avg       0.65      0.71      0.66       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ahrah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Decision Tree Model\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# SVM Model with probability estimates for soft voting\n",
    "svm_model_prob = SVC(random_state=42, probability=True)  # Adjusted SVM initialization\n",
    "svm_model_prob.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Hard Voting Classifier\n",
    "voting_clf_hard = VotingClassifier(estimators=[\n",
    "    ('lr', lr_model),\n",
    "    ('dt', tree_model),\n",
    "    ('svc', svm_model_prob)],  # Using the SVM model with probabilities\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf_hard.fit(X_train_scaled, y_train)  # Training hard voting classifier\n",
    "\n",
    "# Soft Voting Classifier\n",
    "voting_clf_soft = VotingClassifier(estimators=[\n",
    "    ('lr', lr_model),\n",
    "    ('dt', tree_model),\n",
    "    ('svc', svm_model_prob)],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf_soft.fit(X_train_scaled, y_train)  # Training soft voting classifier\n",
    "\n",
    "# Making predictions and evaluating hard voting classifier\n",
    "voting_pred_hard = voting_clf_hard.predict(X_test_scaled)\n",
    "voting_hard_accuracy = accuracy_score(y_test, voting_pred_hard)\n",
    "voting_hard_report = classification_report(y_test, voting_pred_hard)\n",
    "\n",
    "# Making predictions and evaluating soft voting classifier\n",
    "voting_pred_soft = voting_clf_soft.predict(X_test_scaled)\n",
    "voting_soft_accuracy = accuracy_score(y_test, voting_pred_soft)\n",
    "voting_soft_report = classification_report(y_test, voting_pred_soft)\n",
    "\n",
    "print(\"하드보팅의 정확도 : \", voting_hard_accuracy)\n",
    "print(voting_hard_report)\n",
    "print(\"소프트보팅의 정확도 : \", voting_soft_accuracy)\n",
    "print(voting_soft_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a87f21",
   "metadata": {},
   "source": [
    "---\n",
    "## 배깅(Bagging), 페이스팅(Pasting) : 앙상블 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc2fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up the bagging classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=100, \n",
    "    bootstrap= True, \n",
    "    n_jobs=-1 # Use all cores\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up the pasting classifier\n",
    "pasting_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=100, \n",
    "    bootstrap= False, \n",
    "    n_jobs=-1 # Use all cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45fe0a",
   "metadata": {},
   "source": [
    "---\n",
    "## *과제4*\n",
    "***1. 위 코드를 보고, 사용된 기반 분류기의 종류를 서술하시오.***\n",
    "\n",
    "***2. 위 코드를 보고, Bagging과 Pasting 생성에 다른 값을 가지는 parameter는 무엇인지 서술하시오.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbce00a",
   "metadata": {},
   "source": [
    "***답*** : \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f37cd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(bootstrap=False, estimator=DecisionTreeClassifier(),\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(bootstrap=False, estimator=DecisionTreeClassifier(),\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(bootstrap=False, estimator=DecisionTreeClassifier(),\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_clf.fit(X_train_scaled, y_train)\n",
    "pasting_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49cf14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bagging = bagging_clf.predict(X_test_scaled)\n",
    "y_pred_pasting = pasting_clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging); bagging_report = classification_report(y_test, y_pred_bagging)\n",
    "pasting_accuracy = accuracy_score(y_test, y_pred_pasting); pasting_report = classification_report(y_test, y_pred_pasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d44be46c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배깅의 정확도 :  0.7415730337078652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.95      0.85       130\n",
      "           2       0.50      0.14      0.22        36\n",
      "           3       0.60      0.25      0.35        12\n",
      "\n",
      "    accuracy                           0.74       178\n",
      "   macro avg       0.62      0.45      0.47       178\n",
      "weighted avg       0.70      0.74      0.69       178\n",
      "\n",
      "페이스팅의 정확도 :  0.7303370786516854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.95      0.84       130\n",
      "           2       0.38      0.08      0.14        36\n",
      "           3       0.60      0.25      0.35        12\n",
      "\n",
      "    accuracy                           0.73       178\n",
      "   macro avg       0.58      0.43      0.44       178\n",
      "weighted avg       0.67      0.73      0.67       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"배깅의 정확도 : \", bagging_accuracy)\n",
    "print(bagging_report)\n",
    "print(\"페이스팅의 정확도 : \", pasting_accuracy)\n",
    "print(pasting_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c29e4",
   "metadata": {},
   "source": [
    "---\n",
    "## 부스팅(Boosting) : 앙상블 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83395bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy:  0.7191011235955056\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.89      0.82       130\n",
      "           2       0.40      0.22      0.29        36\n",
      "           3       0.67      0.33      0.44        12\n",
      "\n",
      "    accuracy                           0.72       178\n",
      "   macro avg       0.61      0.48      0.52       178\n",
      "weighted avg       0.68      0.72      0.69       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create AdaBoost classifier with a decision tree as base estimator\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=3), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the training set\n",
    "ada_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada = ada_clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
    "ada_report = classification_report(y_test, y_pred_ada)\n",
    "\n",
    "print(\"AdaBoost Classifier Accuracy: \", ada_accuracy)\n",
    "print(ada_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f595f072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Accuracy:  0.7471910112359551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.87      0.84       130\n",
      "           2       0.50      0.47      0.49        36\n",
      "           3       0.75      0.25      0.38        12\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.69      0.53      0.57       178\n",
      "weighted avg       0.74      0.75      0.73       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create Gradient Boosting classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model on the training set\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_report = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "print(\"Gradient Boosting Classifier Accuracy: \", gb_accuracy)\n",
    "print(gb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c144378",
   "metadata": {},
   "source": [
    "---\n",
    "## GridSearchCV : 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a15d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(bootstrap=False, estimator=DecisionTreeClassifier(),\n",
      "                  max_features=0.7, max_samples=200, n_estimators=500,\n",
      "                  n_jobs=-1)\n",
      "optimized_bagging accuracy:  0.7696629213483146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.98      0.86       130\n",
      "           2       0.70      0.19      0.30        36\n",
      "           3       0.75      0.25      0.38        12\n",
      "\n",
      "    accuracy                           0.77       178\n",
      "   macro avg       0.74      0.47      0.51       178\n",
      "weighted avg       0.76      0.77      0.72       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_samples': [50, 100, 200],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': [0.4, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(), n_jobs=-1), \n",
    "                           param_grid, cv=5)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_bagging_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best estimator on the test set\n",
    "y_pred = best_bagging_clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred)\n",
    "bagging_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(best_bagging_clf)\n",
    "print(\"optimized_bagging accuracy: \",bagging_accuracy)\n",
    "print(bagging_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba7969",
   "metadata": {},
   "source": [
    "---\n",
    "## *과제5 T/F*\n",
    "\n",
    "***1. 위 코드와 같이 모델의 성능을 최대화화기 위해 설정값들의 최적 조합을 찾는 과정을 하이퍼파라미터 최적화라고 한다.***\n",
    "\n",
    "***2. 위 배깅/페이스팅 최적화의 parameter 중 max_samples는 무작위로 추출하는 샘플의 수로, 정수만 설정할 수 있다.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6e7b1",
   "metadata": {},
   "source": [
    "***답*** :\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
