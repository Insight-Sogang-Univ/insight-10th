{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49071481",
   "metadata": {},
   "source": [
    "# 1. 경사하강법\n",
    "\n",
    "## 1) 원리\n",
    "- 미분값은 움직일 방향 제시, 현재 값 - 미분 값 \n",
    "미분 값 음수(=현재 값이 최솟값의 왼쪽에 위치)일 때 > 오른쪽으로 이동 <br>\n",
    "미분 값 양수(=현재 값이 최솟값의 오른쪽에 위치)일 때 > 왼쪽으로 이동 <br>\n",
    "- 최솟값에 도달 시 미분값 = 0, 추가적인 변화 발생 X\n",
    "![DFDFDFG.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4de80b11-4b94-4baf-b0ad-756460d3a589/DFDFDFG.png)\n",
    "\n",
    "## 2) 학습률\n",
    "- 방향을 따라 얼마나 움직일지 결정\n",
    "- 학습 = 오차의 최솟값을 찾아감 <br>\n",
    "적당한 학습률을 찾는 것이 중요. 너무 크다면 모델이 발산, 너무 작다면 수렴까지 긴 시간 필요\n",
    "\n",
    "## 3) 경사하강법 문제점\n",
    "- Local Minima 문제<br>\n",
    "여러개의 최솟값을 갖는 경우, global minimum이 아닌 기울기가 0을 갖는 다른 local minimum에 도달하는 경우가 있음. 이를 탈출하는 데에는 여러 알고리즘이 존재\n",
    "<br>\n",
    "- step size 찾기의 문제<br>\n",
    "학습률이 너무 클 때 > 다른 곳으로 발산<br>\n",
    "학습률이 너무 작을 때 > 수렴하기 까지 너무 오랜 시간 필요\n",
    "<br>\n",
    "\n",
    "## 4) 해결법 \n",
    "학습률 지속적으로 바꾸는 **Adaptive Gradient Descent** 사용<br>\n",
    ": 기울기에 관성 부여 - 작은 기울기 쉽게 넘어가도록 하여 local Minimum 탈출 유도<br>\n",
    "**모멘텀**을 사용하지 않으면 작은 기울기를 갖는 구간 탈출에 오랜 시간 소요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8d0a4",
   "metadata": {},
   "source": [
    "# 2. 규제선형모델\n",
    "## 1) 개요\n",
    "- 선형회귀 모델에서는 특성에 곱해지는 계수 조정 <br>\n",
    "좋은 머신러닝 회귀 모델 = 회귀 계수의 과도한 증가 제한할 수 있어야 함 <br>\n",
    "=> 비용 함수 사용<br>\n",
    "- 비용함수 <br>\n",
    "RSS 최소화 방법 - 회귀 계수 값 커지지 않도록 하는 방법의 균형 필요 <br>\n",
    "    - RSS 최소화 방법 ; 학습 데이터의 잔차 오류 값 최소화 \n",
    "\n",
    "- 최적 모델을 위한 비용 함수 목적 \n",
    "Min(학습 데이터 잔차 오류 값 최소화 + 회귀 계수 크기 제어)\n",
    "!https://blog.kakaocdn.net/dn/1c1hC/btqLq25Mok2/ZiVNIIgra3PbUOmbh3gmhk/img.png\n",
    "$\\alpha$ : 회귀 계수 값 제어하는 튜닝 파라미터<br>\n",
    "W : 회귀 계수\n",
    "<br>\n",
    "- 비용 함수 최소화를 위해서는 \n",
    "1. $\\alpha$ = 0일 때 W가 커도 비용함수 = Min(RSS(W))임\n",
    "2. $\\alpha$ = inf일 때, '$\\alpha$ * $||W||^2_2$'도 무한대이므로 W를 0에 가깝게 만들어야 함<br>\n",
    "=> $\\alpha$를 0부터 증가 시키면, W 크기 감소시킬 수 있음.<br>\n",
    "=> $\\alpha$에 페널티를 줘 W 크기 감소해 과적합 개선 = **규제**<br>\n",
    "\n",
    "- 규제\n",
    "1. L1 규제 = 라쏘 회귀 : W의 절댓값에 페널티 부여 \n",
    "2. L2 규제 = 릿지 회귀 : W의 제곱에 페널티 부여\n",
    "3. 엘라스틱넷 회귀 : L1 + L2 결합\n",
    "\n",
    "## 2) 규제 선형 모델 종류\n",
    "### [1] 릿지 회귀 = L2 규제\n",
    "계수 제곱 기준으로 규제 적용 : $\\alpha$ 값을 통해 규제 조절\n",
    "- $\\alpha$ 큼 ; 규제 정도 함께 증가하므로 계수 값 줄여 과소적합 유도\n",
    "- $\\alpha$ 작음 ; 계수 줄이는 역할 감소, 다중 선형회귀 모델과 유사 > 과대적합 가능성 O\n",
    "\n",
    "\n",
    "### [2] 라쏘 회귀 = L1 규제\n",
    "계수의 절댓값 기준으로 규제 적용<br>\n",
    "불필요한 회귀 계수 감소시켜 0으로 만들고 제거 <br>\n",
    "회귀 계수 0으로 만들기 가능 > 유용한 특성 고를 때 사용 \n",
    "\n",
    "### [3] 엘라스틱넷 회귀 = L1 + L2\n",
    "$Min(RSS(W) + \\alpha2*||W|| + \\alpha1*||W||)$ 하는 W찾기\n",
    "\n",
    "- 라쏘 회귀 ; 중요 피처 제외한 나머지를 0으로 만드는 특성으로 인해 알파 값에 따라 회귀 계수 값 변동될 가능성 큼 <br>\n",
    "- 이러한 가능성 완화를 위해 라쏘 회귀에 L2 규제(릿지 회귀) 추가함.<br>\n",
    "- 수행 시간이 오래 걸린다는 단점 \n",
    "\n",
    "#### 주요 파라미터\n",
    "a * L1 + b * L2<br>\n",
    "a : L1규제의 알파 값<br>\n",
    "b : L2규제의 알파 값<br>\n",
    "0.5일 때 RMSE가 가장 좋으며, 라쏘보다 다른 변수에 대한 알파값의 영향이 적음\n",
    "\n",
    "## 3) 편향과 분산의 trade-off 관계\n",
    "- 과소 적합 = 높은 편향, 낮은 분산<br>\n",
    "모델 복잡도 넢이고, 다양한 특성 사용\n",
    "<br>\n",
    "- 과대 적합 = 낮은 편향, 높은 분산<br>\n",
    "데이터 수 늘리거나 특성 선택/추출, 규제를 통해 복잡도 낮추기 <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc3c46",
   "metadata": {},
   "source": [
    "# 3. 스케일링\n",
    "## 1) 정의\n",
    "- 데이터 전처리 중 일부, 모든 피쳐의 분포/범위 동일하게 조정<br>\n",
    "- 모든 특성의 범위를 같게 해 머신러닝이 잘 작동하게끔 함 <br><br>\n",
    "\n",
    "### 주요 개념\n",
    "- 표준화<br>\n",
    "정규 분포를 표준 정규 분포(평균 0, 분산 1)로 변환함<br>\n",
    "데이터가 평균에서 얼마나 떨어져 있는지 나타내며, 특정 범위 밖에 있는 데이터는 이상치로 간주 후 제거 <br><br>\n",
    "\n",
    "- 정규화<br>\n",
    "입력된 X값을 모두 0과 1사이의 값으로 변환해 서로 다른 피쳐의 크기 통일 <br>\n",
    "데이터의 상대적 크기에 대한 영향을 줄이기 위함이며, 비교에 용이<br>\n",
    "\n",
    "## 2) 주의사항\n",
    "### 사용 라이브러리 및 단어 \n",
    "- 사이킷 런 ; 머신러닝용 라이브러리<br>\n",
    "- 학습 데이터 ; 모델 학습에 사용하는 데이터<br>\n",
    "- 테스트 데이터 ; 모델 성능 검증 위해 사용하는 데이터<br>\n",
    "보통 전체 데이터를 학습 데이터 : 테스트 데이터 = 70 : 30 으로 분할함<br>\n",
    "\n",
    "### 사용 함수 \n",
    "- fit() : 데이터 변환 기준 정보 설정 적용<br>\n",
    "- transform() : 설정된 정보 통해 데이터 변환<br>\n",
    "- fit_transform() : 두 메서드 동시 적용<br>\n",
    "- fit()은 학습 데이터에만 적용해 먼저 모델 학습시킨 후, transform()을 통해 학습/테스트 데이터 스케일 조정<br>\n",
    "- 학습 데이터 세트로 fit()을 한 결과에 transform() 적용<br>\n",
    "fit_transform()은 테스트 데이터에 사용 불가 / fit() 사용 시 스케일링 기준 정보 달라지므로 테스트 데이터에는 사용하면 안 됨<br>\n",
    "- 타겟 데이터에는 스케일링 진행 X<br>\n",
    "타겟데이터 = 예측하려는 출력 값 <br>\n",
    "\n",
    "## 3) 스케일링 종류 \n",
    "### [1] StandardScaler()\n",
    "- 피처의 평균 0, 분산1로 하는 표준화 수행<br>\n",
    "- 이상치 유무, 데이터 분포 고려 <br>\n",
    "- 선형모델, 신경망에 적합<br>\n",
    "- 회귀보단 분류<br>\n",
    "\n",
    "### [2] MinMaxScaler()\n",
    "- 모든 피처 값을 0과 1사이로 변환<br>\n",
    "- 데이터 최댓값/최솟값 알 때 사용<br>\n",
    "- 이상치 존재 시 이상치가 극값이 되어 데이터가 좁은 범위에 분포<br>\n",
    "- 분류보단 회귀<br>\n",
    "\n",
    "### [3] MaxAbsScaler()\n",
    "- 데이터가 -1과 1 사이에 위치하도록 변환<br>\n",
    "- 절댓값의 최솟값이 0, 최댓값이 1이 됨<br>\n",
    "- 데이터가 양수만 있을 경우 MinMaxScaler()와 유사<br>\n",
    "- 큰 이상치에 민감<br>\n",
    "\n",
    "### [4] RobustScaler()\n",
    "- 데이터 중앙값이 0, IQE(Q3 - Q1) = 1 이 되도록 스케일링<br>\n",
    "- 이상치 값의 영향을 덜 받음<br>\n",
    "- 표준화 후 데이터 분포가 더 넓음 <br>\n",
    "\n",
    "---------------**열** 대상 스케일러------------------------\n",
    "\n",
    "### [5] Normalizer()\n",
    "- 데이터 포인트(**행**)의 크기를 1로 만들어줌 <br>\n",
    "- 행마다 정규화 진행<br>\n",
    "- 데이터 포인트의 상대적 크기 비교에 유용 <br>\n",
    "- 텍스트 분류, 추천시스템에 사용<br>\n",
    "\n",
    "### 스케일링 시 고려할 점 \n",
    "- 이상치 제거 후 스케일링 진행<br>\n",
    "- 모든 피처의 분포를 동일하게 만들 필요 없음<br>\n",
    "- 상황에 맞는 적절한 방법을 사용할 줄 알아야 함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbee5d",
   "metadata": {},
   "source": [
    "# 4. 차원 축소\n",
    "## 1) 차원\n",
    "- 수학 공간 내에 있는 것의 위치를 나타낼 때 필요한 축의 개수 <br>\n",
    "- 사용된 수 = 공간의 매개 변수 <br>\n",
    "- 차원 = 변수의 개수<br>\n",
    "- 차원 클수록 시각화/이해/분석 어려움<br>\n",
    "\n",
    "## 2) 차원 축소 알고리즘\n",
    "### [1] PCA 주성분 분석\n",
    "- 목적 : n개의 변수로 전체 변수의 분산에 대한 설명이 가능하다면 추가 변수 사용할 필요X <br>\n",
    "- 고차원 데이터 > 기존 분산 보존하는 새로운 선형 독립 변수로 변환 <br>\n",
    "- 변수 간 상관관계 이용해 대표하는 주성분 추출, 차원 축소<br>\n",
    "- 시각화에 사용<br>\n",
    "\n",
    "### [2] LDA 선형 판별 분석\n",
    "- 저차원 공간에 투영해 차원 축소<br>\n",
    "- 개별 클래스 분별 기준 최대한 유지 <br>\n",
    "- 입력 데이터의 결정값 클래스를 최대한 분리할 수 있는 축 <br>\n",
    "- 클래스 간 내부 분산과 내부 분산 비율 최대화 = 클래스 간 분산 최대화, 클래스 내부 분산 최소화<br>\n",
    "\n",
    "### [3] SVD 특이값 분해\n",
    "- 다양햔 형태의 행렬 분해 <br>\n",
    "\n",
    "### [4] NMF 비음수 행렬 분해\n",
    "- 하나의 객체 정부를 음수를 포함하지 않은 두 개의 부분 정보로 인수분해<br>\n",
    "- 의미 특징과 의미 변수로 나눠 효율적으로 표현 가능<br>\n",
    "\n",
    "\n",
    "## 3) 차원 축소의 필요성\n",
    "### [1] 차원의 저주 문제\n",
    "- 차원의 저주 : 차원 커짐 > 공간 범위 증가 > 더 많은 샘플 필요한 현상<br>\n",
    "- 고차원 데이터 분석 ; 많은 변수 저장과 시간 필요<br>\n",
    "\n",
    "### [2] 과적합 문제\n",
    "- 너무 정교해 일반화 결과 도출에 실패할 가능성 존재<br>\n",
    "- 차원 늘려갈수록 데이터 복잡성 증가, 효율성 감소<br>\n",
    "\n",
    "### [3] 분석/시각화 용이\n",
    "- 낮은 차원일수록 구조 파악 / 시각화 쉬움 <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be146838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
