#회귀심화 예습

**1.경사하강법(Gradient Descent)**
모델이 학습하는 방법임
파라미터 벡터 세타에 대해 cost function의 현재 기울기를 계산,
기울기가 감소하는 방향으로 진행(*이 방향을 결정*)
기울기가 0이 되면 최솟값에 도달

현재값 < 최솟값
현재값-미분값을 하면 커짐(점이 오른쪽으로 이동)
현재값 > 최솟값
현재값-미분값을 하면 작아짐(점이 왼쪽으로 이동)
최종적으로 점은 최솟값에 도달하게 됨

*미분 값은 방향만 결정, 학습률이 얼마나 움직일지의 정도를 결정해줌*

학습률=Learning Rate
너무 크면 모델은 수렴하지 못하고 발산
너무 작으면 모델 수렴까지의 시간이 오래 걸리게 됨

경사하강법의 문제점
1. local minimum
cost function의 모양에 따라 global minimum이 아닌 local minimum에 도달하게 될 수도 있음
convex함수가 아니라면 주의해야한다.
deep learning의 경우 cost function 예측이 어려워 local minimum에 빠지기 쉽고, 탈출해야함.
2. 학습률의 크기를 찾기가 어려움
-Adaptive Gradient Descent(학습 도중 학습률을 지속적으로 바꿈)

->모멘텀: 기울기에 관성을 부여, 작은 기울기는 넘어갈 수 있도록함



**2.규제선형모델**
모델이 훈련세트에 과대적합되지 않도록 규제를 가하고자 등장한 모델
특성에 곱해지는 계수의 크기를 조정한다. 
-> 데이터에 적절히 적합하면서, 회귀계수가 기하급수적으로 커지는 것(과대적합)을 방지할 수 있게 됨

최적모델을 위한 cost 함수 목표 = Min(학습데이터 잔차오류 최소화+회귀계수 크기 제어)

L2 규제: W의 제곱에 대해 페널티를 부여(릿지 회귀) (Alpha 매개변수 값을 통해 규제의 정도를 조절, 알파가 커지면 규제 정도가 세져 과소적합됨, 알파가 작아지면 다중선형회귀모델과 유사해져 과대적합됨)

L1 규제: W의 절댓값에 대해 페널티를 부여(라쏘 회귀) (불필요한 회귀 계수를 급격히 0으로 만들고 제거, 유용한 특성 골라냄, Feature 선택 특성)

L2 규제 + L1 규제: 엘라스틱 넷 회귀(라쏘 회귀가 alpha값에 따라 회귀 계수의 값을 급격히 변동시킴에 따라 이를 완화하기 위해 L2규제를 추가함. 수행시간이 상대적으로 오래 걸린다.)



**3.스케일링(Scaling)**
데이터 전처리 과정 중 하나, 모든 피쳐들의 데이터 분포나 범위를 동일하게 조정하는 과정
표준화: 값들의 정규분포를 평균이 0이고 분산이 1인 표준 정규 분포로 변환하는 것
정규화: 값들을 모두 0과 1 사이의 값으로 변환하여 피쳐의 크기를 통일하는 개념
주의사항: 사이킷런을 사용, 전체 데이터셋을 (70학습,30테스트)로 분할하여 사용, fit_transform()은 테스트 데이터에 사용 불가,fit()도 테스트 데이터에 적용하면 안됨
전체 데이터에 스케일링 변환을 적용하고 이후 학습, 테스트 데이터를 분리하는 것을 추천

종류:
1. standardScaler(): 이상치에 민감함, 회귀보다 분류에 유용함, 선형모델이나 신경망에서 잘 작동
2. MinMaxScaler(): 모든 값이 0과 1 사이의 데이터값을 갖도록 변환, 해석 용이, 딥러닝에 사용, 데이터의 최소최대 알때 사용, 분류보다 회귀에 유용함
3. MaxAbsScaler(): 데이터가 -1과 1 사이의 데이터값을 갖도록 변환,절댓값의 최소값이 0, 데이터 값이 양수면 MinMaxScaler와 유사하게 동작, 큰 이상치에 민감
4. RobustScaler(): 데이터의 중앙값이 0, IQR=1이 되도록 스케일링(중앙값과 사분위값 사용),이상치 영향 최소화, 표준화 후 데이터가 더 넓게 분포
5. Normalizer(): 각 데이터의 크기를 1로 변환, 각 행마다 정규화가 진행됨,데이터의 분포 변경X, 상대적 크기를 비교, 텍스트 분류나 추천 시스템에 사용


**4.차원축소**
차원: 수학에서 공간 내에 있는 점 등의 위치를 나타내기 위해 필요한 축의 개수
차원축소: 차원이 크다=변수가 많다. 차원이 크면 시각화가 어렵고 이해,분석이 어려움
차원축소알고리즘: PCA, LDA, SVD, NMF

PCA(주성분분석,Principal Component Analysis)
: 전체 변수 중 몇개로만 전체 변수의 분산을 설명함
여러 변수간 존재하는 상관관계를 이용해 주성분을 추출해 차원 축소
축 회전/상관관계 사용/시각화를 위해 사용/다차원의 시각화 가능/정보의 유실 발생 가능/분산이 가장 넓은 지역을 찾은 후 그 지역을 직선으로 표시해 점이 퍼져있는 정도를 지켜준다/입력 데이터 변동성의 가장 큰 축 추출

LDA(선형판별분석, Linear Discriminant Analysis)
: PCA와 비슷하나 지도학습의 분류에서 사용하기 쉽도록 개별 클래스 분별 기준을 최대한 유지
입력 데이터의 결정값 클래스를 최대한 분리할 수 있는 축 추출/클래스간 분산 크게 클래스 내부 분산 작게

SVD(특이값 분해, Singular Value Decomposition)
: 정사각행렬이 아닌 m*n형태의 다양한 행렬 분해(특이값 분해)

NMF(비음수 행렬 분해, Non-negative Matrix Factorization)
: 하나의 객체정보를 음수를 포함하지 안흔 두개의 부분정보로 인수분해
대량의 정보를 의미특징과 의미변수로 나누어 표현할 수 있음/ 단어 기반으로 특성 추출하는 예시

차원축소의 필요성:
1. 차원의 저주(표본이 많이 필요함,시간이 많이 필요함)
2. 과적합(overfit, 일반화 결과를 도출할 수 없음, 복잡성 증가,효율성감소)
3. 분석 및 시각화 용이(군집성 찾아내는 등의 과정이 좋음)

