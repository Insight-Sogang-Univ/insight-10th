[1] 경사 하강법
: 이차함수 형태의 손실함수에서 최솟값을 갖는 점을 기준으로 x값이 왼쪽에 있다면 "현재값-미분값(음수)"을 하면 "현재값"보다 커지게 되면서 오른쪽으로 점이 이동한다. 반대로 x값이 최솟값보다 오른쪽에 있다면 "현재값-미분값(양수)"을 하면 "현재값"보다 작아지면서 왼쪽으로 점이 이동한다. 
이 과정을 반복하다 보면 최솟값을 기준으로 왼쪽에 있는 값들은 점점 오른쪽으로 이동하고, 오른쪽에 있는 값들은 점점 왼쪽으로 이동하게 되면서 결국 최종적으로는 최솟값에 도달한다는 원리가 경사 하강법의 기본 원리이다.

-학습률(Learning Rate)
그렇다고 미분값만 빼주면 저절로 최솟값에 도달하는 것은 아니다. 오히여 최솟값에서 멀어지는 경우도 발생하는데, 결국 미분값은 이동 방향만 결정할 뿐 '얼마나 움직일지'는 학습률이 결정한다.

- 경사하강법의 문제점
1) Local Minima 문제
모든 cost function이 하나의 최솟값만을 갖는 convex function이라는 보장이 없는 상황에서는 global minimum이 아닌 local minimum에 도달하게 되는 문제가 발생할 수 있다.

2) 적절한 step size 찾지 못하는 문제
학습률이 너무 크게 설정되면 최적값에 수렴하지 못하고 발산하는 문제가 발생할 수 있다. 반대로 너무 작으면 최적값에 수렴하는 시간이 오래 걸리는 문제도 발생할 수 있다.

- 그에 대한 해결법
학습률을 지속적으로 변경하는 'Adaptive Gradient Descent' 이용, 모멘텀을 이용하여 local minimum을 쉽게 탈출할 수 있도록 함


[2] 규제선형모델
: 모델이 학습 데이터에 overfitting되지 않도록 규제를 가하고자 등장한 모델이다. 선형 회귀 모겔에서는 회귀계수의 크기를 조정하는 것으로, 규제선형모델을 통해 회귀 모델의 RSS를 최소화하느라 회귀계수가 쉽게 커지는 경향(overfitting)을 조절할 수 있다. 

1) 릿지 회귀
회귀 계수의 제곱을 비용 함수에 추가하는 방식인 L2 규제를 사용

2) 라쏘 회귀
회귀 계수의 절댓값을 비용 함수에 추가하는 방식인 L1규제를 사용

3) 엘라스틱넷 회귀
L1과 L2를 결합한 회귀 방식


[3] Scaling
데이터 전처리 과정에서 모든 feature들의 데이터 범위(분포)를 동일하게 하도록 조정하는 과정

1) StandardScaler()
특성의 평균을 0, 분산을 1로 조정하는 표준화를 수행하는 스케일링

2) MinMaxScaler()
모든 특성들이 0과 1 사이의 값을 갖도록 변환하는 스케일링

3) MaxAbsScaler()
데이터가 -1과 1사이의 범위에 위치하도록 조정하는 스케일링

4) RobustScaler()
데이터의 중앙값이 0, 사분위수 범위가 1이 되도록 조정하는 스케일링

5) Normalizer()
각 데이터 행의 크기를 1로 변환해주는 스케일링 (즉, 각 행마다 정규화가 진행된다)


[4] 차원축소
데이터에서 차원은 변수의 수를 의미하므로, 차원이 너무 크면 변수가 너무 많다는 점 때문에 시각화 및 분석이 어려워진다는 문제가 발생한다. 따라서, 이 차원을 줄이는 기법을 차원 축소라고 부르는 것이다.
1) PCA(주성분 분석)
축을 회전시켜 새로운 축을 찾아내어 차원을 줄이는 기법

2) LDA(선형판별분석)
PCA처럼 입력 데이터를 저차원 공간에 투영해 차원을 축소하지만, PCA와는 달리 개별 틀래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소하는 기법

3) SVD(특이값 분해)
정사각형 행렬이 아닌 다양한 형태의 행렬을 분해하여 차원을 축소

4) NMF(비음수 행렬 분해)
하나의 객체정보를 음수를 포함하지 않는 2개의 부분 정보로 인수분해하여 차원을 축소하는 기법
