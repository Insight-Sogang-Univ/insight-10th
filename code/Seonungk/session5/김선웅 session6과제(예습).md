# 경사하강법

*회귀 모델에서 기울기가 0이 되는 방향으로 점차 나아가는 학습법<br>

손실함수를 이용하는데, 손실함수는 실제값과 예측값의 차이를 비교하는 함수. 목적함수<br>라고도 부른다. 손실함수의 최솟값을 구하는 것이 목적

## 학습률
- 방향이 정해져도 얼마나 갈지가 중요하므로 학습률 설정 필요
- learning rate가 얼만큼을 의미. 내려가는 보폭으로 받아들이면 이해가 쉬움.

## 경사하강법의 문제점
1. Local Minima 문제: 모든 cost functioin이 하나의 최솟값을 가지는 매끈한 곡선 형태가
아니므로, global이 아닌, local에 도달하면 탈출해야 함.
2. 적절한 크기의 step size를 찾지 못해 생기는 문제: 적절한 보폭으로 내려가지 않으면 최저점을 찾지 못하고 그 언저리를 계속 돌게 됨.

## 해결법
1. adaptive gradient descent 이용(학습 도중 학습률을 지속적으로 바꾼다.)
2. 모멘텀: 기울기에 관성을 부여하여 작은 기울기 쉽게 넘어가게 함.

# 규제선형모델

*모델이 훈련세트에 과대적합되지 않도록 규제를 가한다.
선형 회귀 모델에서는 특성에 곱해지는 계수의 크기를 조정

## 규제 방식의 종류(릿지 회귀, 라쏘 회귀, 엘라스틱넷 회귀)

1. 릿지 회귀: 계수를 제곱한 기준으로 규제를 적용(L2규제)
-alpha 매개변수 값을 통해 규제의 정도를 조절할 수 있다.
2. 라쏘 회귀: '계수의 절댓값 기준'으로 규제를 적용(L1규제)
- L1규제를 선형 회귀에 적용한 것이 라쏘 회귀. L2규제가 회귀 계수의 크기를 감소시키는데 반해, L1규제는 불필요한 회귀 계수를 급격하게 감소시킨다.
3. 엘라스틱넷 회귀: L2규제와 L1규제를 결합한 회귀
- L2규제를 Lasso 회귀에 추가한 것.수행시간이 오래 걸린다는 단점이 있다.

# 편형과 분산의 trade-off관계
1. 과소적합: 높은 편향, 낮은 분산
- 모델 복잡도를 높이고 더 많고 다양한 특성과 모델 시도해야 함.
2. 과대적합: 낮은 편향, 높은 분산
- 더 많은 데이터를 수집하거나,특성 선별하여 모델 복잡도를 줄여야 함.

# 스케일링
***스케일링이란? : 모든 피쳐들의 데이터 분포나 범위를 동일하게 조정하는 과정

## 표준화: 평균0, 분산 1인 표준정규 분포로 변환

- 데이터가 평균으로부터 얼마나 떨어져 있는지 나타낸다. 데이터가 평균으로부터 얼마나 떨어져 있는지 나타낸다.

## 정규화: 입력된 값들을 모두 0과 1사이의 갓으로 변환해 서로 다른 피쳐의 크기를 통일하는 개념

- 가장 큰 값이 1이고 가장 작은 값은 0으로 변환

# 스케일링 변환 시 주의사항

-사이킷런 라이브러리를 사용하는데, 테스트 데이터로 다시 새로운 스케일링 기준을 만들어 버리면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 달라지기 때문에, 테스트 데이터에는 fit() 적용하면 안된다.

## 스케일링 종류
* standardscaler(): 특성의 평균 0, 분산 1로 맞추는 표준화 수행
- 회귀보다 분류에 유용하며, 선형 모델보다는 신경망과 같은 알고리즘에서 잘 작동

* MinMaxScaler(): 가장 작은 값은 0, 가장 큰 값은 1로 변환
- 데이터의 범위를 알 때 사용, 이상치에 민감하며 분류보다 회귀에 유용

* MaxAbsScaler(): 데이터가 -1과 1 사이에 위치하도록 스케일링
- 큰 이상치에 민감하며 데이터 값이 양수이면 minmaxscaler와 유사하게 작동

* RobustScaler(): 데이터의 중앙값이 0, IQE(Q3-Q1) =1이 되도록 스케잉ㄹ링
- 이상치 영향이 적다

* Normalizer(): 각 데이터 포인트(행)의 norm(크기)을 1로 만들어준다.
- 각 행 마다 정규화가 진행되어, 데이터의 분포를 변경하지 않고 각 데이터 포인트의 상대적인 크기를 비교하는데 유용.
- 텍스트 분류나 추천 시스템 등의 문제에서 사용한다.

# 차원 축소
*차원이란? : 공간내에 있는 점 등의 위치를 나타내기 위해 필요한 축의 개수

## 차원 축소 알고리즘
1. PCA(주성분 분석): 기존의 기존의 분산을 최대한 보존하는 선형 독립의 새로운 변수들로 변환한다.
- 축을 회전시켜 새로운 축을 찾아내며, 몇 개의 점이 서로 겹쳐 정보의 유실이 있을 수 있음. 그러므로 분산이 가장 넓은 지역을 찾은 후 그 지역을 기준으로 분산 유지한다.

2. LDA(선형 판별 분석): PCA와 유사하지만, 입력 데이터의 결정값 클래스를 최대한 분리할 수 있는 축을 차자 클래스 간 분산은 최대한 크게, 클래스 내 분산은 최대한 작게 한다.
- 분류에서 많이 사용된다.

3. NMF(비음수 행렬 분해): 하나의 객체정보를 음수를 포함하지 않은 두 개의 부분 정보로 인수분해하는 방법
- 대량의 정보를 의미 특징과 의미 변수로 나누어 효율적으로 표현할 수 있다.

## 차원 축소의 필요성
1. 차원의 저주 문제 방지 : 차원이 커지면서 두 데이터 간의 거리가 멀어지고 밀도가 급격히 줄어들어 더 많은 수의 SAMPLE이 필요해 진다.

2. 과적합 문제 방지: 고차원에서 문제를 해결하면 너무 정교해저 일반화가 어려워진다.

3. 분석 및 시각화 용이: 복잡한 형태의 데이터 구조를 낮은 차원에서 분석하여 구조를 파악하기 쉽거나 시각화하기 쉽다.


```python

```
