# 경사하강법

어느 방향으로 갈 것인가를 결정하는 것

1. 원리
    - 2차 함수를 미분할 시, 최종적으로 최솟값에 도달하게 됨
2. 학습률
    - 미분은 방향만 결정하고, 얼마나 갈지는 학습률이 결정
    - 적당한 크기여야 발산하지 않고, 수렴하기에 시간이 오래 걸리지 않음
3. 문제점
    - convex function이 아닐 경우, local minimum에 빠지지 않아야 함
    - 학습률의 적당한 step size를 찾지 못함
4. 해결법
    - 학습 도중 학습률을 지속적으로 바꾸는 Adaptive Gradient Descent 이용

# 규제선형모델

1. 개요
    - 모델이 훈련세트에 과대적합하지 않도록 규제를 가함 (선형 계수의 크기 조정)
    - 기존 선형 모델 비용 함수는 실제 값과 예측값 차이를 최소화하다보니, 변동성이 심해져도 데이터 세트에서는 예측 성능이 저하되기 쉬움
    - 비용 함수는 RSS 최소화 방법과 과적합을 방지하기 위한 계수값이 커지지 않도록 하는 방법이 균형을 이뤄야 함 

2. 규제 방식의 종류
    - 최적 모델을 위한 Cost 함수 목표 = Min (학습데이터 잔차 오류 최소화 + 회귀계수 크기 제어)
    - 규제 = 알파 값으로 패널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식
        - 릿지 회귀 = L2 규제 = W의 제곱에 대해 패널티를 부여하는 방식
        - 라쏘 회귀 = L1 규제 = 절댓값에 대해 패널티를 부여하는 방식
        - 엘라스틱넷 회귀 = L2 규제와 L1규제를 결합한 회귀
        
        <br>
        1. 릿지 회귀
            = 계수를 제곱한 기준으로 규제를 적용 (L2 규제)
            = 알파 매개변수 값을 통해 규제 정도 조절 가능 (증가 : 과소적합)
            ex) 0.001부터 100까지 10배씩 늘려가며 fitting 정도 확인
            
         <br>
         2. 라쏘 회귀
             = 계수의 절댓값 기준으로 규제를 적용 (L1 규제)
             = 불필요한 회귀 계수를 급격히 감소시켜 0으로 만들고 제거
             
         <br>
         3. 엘라스틱넷 회귀 
             = 라쏘 회귀가 0으로 만듦에 따라 회귀 계수가 급변하는데, 이를 릿지 회귀를 추가한 것
             = 수행 시간이 상대적으로 오래 걸림

# Scaling

1. 개요 <br>= 데이터 전처리 과정 중 하나로, 모든 피쳐들의 데이터 분포나 범위를 동일하게 조정하는 과정
    - 표준화 = 입력된 값들의 정규 분포를 평균이 0이고 분산이 1인 표준 정규 분포로 변환하는 것
    - 정규화 = 입력된 x 값들을 모두 0과 1 사이의 값으로 변환해 서로 다른 피쳐의 크기를 통일하는 것
    
2. 주의 사항
    - 학습 데이터 = 모델이 학습하는데 사용하는 데이터
    - 테스트 데이터 = 모델이 학습된 후에 모델의 성능을 검증하기 위해 사용되는 데이터
    - 테스트 데이터로 다시 스케일링 기준을 만들어 버리면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 달라지기 때문에, fit()를 적용하면 안된다. 따라서 전체 데이터에 스케일링 변환을 적용하고, 이후에 학습/테스트 데이터를 분리하는 것이 좋음
    - 일반적으로 타겟(y)에 대한 스케일링은 진행하지 않음
    - 스케일링은 주로 입력 데이터의 스케일을 조정하는데 사용.
    - 타겟 데이터는 모델을 학습할 때 예측하려는 출력 값을 의미
    
3. 종류
    1. StandardScaler()
        - 특성의 평균을 0, 분산을 1로 맞추는 표준화를 수행
        - 평균과 표준편차가 이상치로부터 영향을 많이 받는다는 점에서 이상치에 민감함
        - 선형 모델이나 신경망과 같은 알고리즘에서 잘 작동됨
        - 회귀보다 분류에 유용함
        
    2. MinMaxScaler()
        - 모든 피쳐들이 0과 1 사이의 데이터 값을 갖도록 변환
        - 다른 스케일러보다 해석이 용이하며, 일반적으로 딥러닝에서 많이 사용
        - 데이터의 최소값과 최대값을 알 때 사용
        - 이상치가 극값이 되어, 스케일링 결과가 매우 좁은 범위로 압출될 수 있는 단점
        - 분류보다 회귀에 유용
        
    3. MaxAbsScaler()
        - 데이터가 -1과 1 사이에 위치하도록 스케일링
        - 데이터의 값이 양수만 존재할 경우 MinMaxScaler와 유사
        - 큰 이상치에 민감할 수 있음
        
    4. RobustScaler()
        - 데이터 중앙값이 0, IQE=1이 되도록 스케일링
        - 사분위값은 평균보다 이상치의 영향을 덜 받아, 이상치 영향을 ㅗ치소화
        - StandardScaler보다 표준화 후 데이터가 더 넓게 분포
        
    5. Normalizer()
        - 각 데이터 포인트(행)의 norm(크기)을 1로 만들어주는 스케일링
        - 앞의 4가지는 열을 대상으로 했으나 Normalizer는 행마다 정규화
        - 데이터 포인트의 상대적인 크기를 비교하는데 유용
        - 텍스트 분류나 추천 시스템 등의 문제에서 사용
        
    *스케일링 기법에서 이상치는 변환 효과를 저해하므로, 제거 후 진행하는 게 좋음

# 차원 축소

1. 차원이란?
    = 수학에서 공간(매개 변수) 내에 있는 점 등의 위치를 나타내기 위해 필요한 축의 개수
    - 차원이 크면 시각화가 어렵고,이해/분석 어려움 >> 차원 축소

2. 차원 축소 알고리즘

    1. PCA (주성분 분석)
        - 고차원 데이터를 기존의 분산을 최대한 보존하는 선형 독립의 새로운 변수로 변환
        - 차원을 줄이기 위해 축을 회전시켜 새로운 축을 찾아냄
        - 여러 변수 간의 상관관계를 이용해 이를 대표하는 주성분을 추출
        - PCA는 시각화를 위해 사용되며, 다차원의 시각화도 가능
        - 점이 겹치면 정보의 유실이 있을 수 있음. 따라서 분산이 넓은 지역을 찾아 점을 옮김
     <br>
     2. LDA (선형 판별 분석)
        - PCA와 유사하나 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하며 차원 축소
        - 입력 데이터의 결정값 클래스를 최대한 분리할 수 있는 축을 찾음
        - 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산과 클래스 내부 분산의 비율을 최대화하는 방식으로 축소
      <br>
      3. SVD (특이값 분해)
          - m*n 형태의 다양한 행렬을 분해
          
      4. NMF (비음수 행렬 분해)
          - 하나의 객체정보를 음수를 포함하지 않은 두 개의 부분 정보로 인수분해
          - NMF는 대량의 정보를 의미 특징과 의미 변수로 나누어 효율적으로 표현할 수 있는 방법



```python

```
